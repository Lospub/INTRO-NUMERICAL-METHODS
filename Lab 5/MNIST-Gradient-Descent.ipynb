{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Lab 5 - Building a Small Neural Network to Recognize Digits</h2>\n",
    "<h4>Demo: November 24; Due date: November 26.</h4>\n",
    "\n",
    "In this lab we will use Gradient Descent to implement a function for recognizing handwritten digits. We will use the MNIST data set to learn this function. Let's download the data set and take a look at a few images. You will have to install tensorflow to download the data set from the code. You should be able to install it with:\n",
    "\n",
    "<i>pip install tensorflow</i>\n",
    "\n",
    "Let one of the instructors know if that does not work for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-16e5ba5ad271>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_digit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_digit(image):\n",
    "    \"\"\"\n",
    "    This function receives an image and plots the digit. \n",
    "    \"\"\"\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# The x variables contain the images of handwritten digits the y variables contain their labels indicating \n",
    "# which digit is in the image. We will see an example of image and label shortly. We have two data sets here:\n",
    "# training and test sets. The idea is that we use the training set to learn the function and then we evaluate \n",
    "# the system on images it did not see during training. This is to simulate the scenario where we build a system\n",
    "# and we use it in the wild, where people write new digits and we would like our system to accurately recognize them.\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Each image is of size 28x28 and the training data set has 60,000 images\n",
    "# the shape of x_train should then be (60000, 28, 28).\n",
    "print(x_train.shape)\n",
    "\n",
    "# Let's take a look at the first training instance. I hope you can recognize the digit 5 in the image.\n",
    "# Feel free to change the index of x_train to see other images. \n",
    "plot_digit(x_train[0])\n",
    "\n",
    "# The y_train structure has shape 60,0000, with one entry for each image. The value of the first\n",
    "# entry of y_train should be a five, indicating that the first image is of a 5.\n",
    "print(y_train.shape)\n",
    "print('Label: ', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using all 60,000 images, we will use \"only\" 20,000 in our experiments to speed up training.\n",
    "training_size = 20000\n",
    "\n",
    "# We will flatten the images to make our implementation easier. Instead of providing \n",
    "# an image of size 28x28 as input, we will provide a vector of size 784, with one entry \n",
    "# for each pixel of the image. We will also normalize the values in the images. The pixels\n",
    "# values in the images vary from 0 to 255. We normalize them to avoid overflow. This will \n",
    "# be clear later once we better understand the learning algorithm.\n",
    "images, labels = (x_train[0:training_size].reshape(training_size,28*28) / 255, y_train[0:training_size])\n",
    "\n",
    "# The flattened images will be kept as column vectors in the matrix images\n",
    "images = images.T\n",
    "print('Shape of flattned images: ', images.shape)\n",
    "\n",
    "# Here we apply the same transformations described above on the test data. \n",
    "images_test = x_test.reshape(x_test.shape[0], 28*28) / 255\n",
    "images_test = images_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training a Model</h3>\n",
    "\n",
    "We will train one function $h_{w_i}$ for each digit $i$, which will be parameterized by a vector $w_i$ and a scalar $b_i$. The vector $w_i$ will contain one entry for each pixel in the flattened image. That way, if an image $x$ contains the digit $i$, then the operation $w_i \\cdot x + b$ should produce a number close to $1$; the operation should produce a number close to $0$ if image $x$ does not contain digit $i$. We define $h_{w_i}$ as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "h_{w_i} = w_i \\cdot x + b\n",
    "\\end{equation*}\n",
    "\n",
    "Given an image $x$ our model predicts digit $i$ for the largest value returned by $h_{w_i}$ for all $i$. For example, if we pass the first image of the training data x_train[0], then the system will predict the digit 5 correctly if the output of the functions $h_{w_i}$ are similar to the following (these are made-up numbers to illustrate the procedure).\n",
    "\n",
    "\\begin{align*}\n",
    "h_{w_0} &= 0.1 \\\\\n",
    "h_{w_1} &= 0.01 \\\\\n",
    "h_{w_2} &= 0.05 \\\\\n",
    "h_{w_3} &= 0.25 \\\\\n",
    "h_{w_4} &= 0.40 \\\\\n",
    "h_{w_5} &= 0.95 \\\\\n",
    "h_{w_6} &= 0.80 \\\\\n",
    "h_{w_7} &= 0.12 \\\\\n",
    "h_{w_8} &= 0.71 \\\\\n",
    "h_{w_9} &= 0.01 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Since $h_{w_5}$ is the largest value, the model will predict that the image is of the digit 5.\n",
    "\n",
    "We will use the training data (variables images and labels) to discover vectors $w_i$ e scalars $b_i$ that classify most of the training data correctly. This will be achieved by defining an error function and then applying gradient descent to minimize the error. We will adjust the values of $w_i$ and $b$ to reduce the error for each function $h$. \n",
    "\n",
    "Let $y$ be the label of an image $x$ for a specific function $h_{w_i}$. $y = 0$ if $x$ does not contain the digit $i$ and $y = 1$ otherwise. The error function $J(w_i)$ is defined as the sum of errors for all $m$ images in the training set.\n",
    "\\begin{equation*}\n",
    "J(w_i) = \\frac{1}{2m} \\sum_{x, y} (h_{w_i}(x) - y)^2\n",
    "\\end{equation*}\n",
    "As an example, if $h_{w_i}(x) = 0.95$ for an image that does not contain digit $i$, then the error for this particular image will be $(0.95 - 0)^2 = 0.9025$; if $h_{w_i}(x) = 0$, then the error would have been zero for this instance. The positive error informs gradient descent that the values of $w_i$ need to be adjusted to reduce the error.  \n",
    "\n",
    "<h3>Gradient Descent</h3>\n",
    "\n",
    "We will initialize the values of $w_i$ and $b$ with zeros and then adjust them according to gradient descent. \n",
    "\n",
    "Let $w_{i, j}$ be the $j$-th value of the vector $w_i$ for digit $i$. The value of $w_{i, j}$ and $b$ will be modified according to gradient descent (see equations below), where $\\alpha$ is a small value indicating the size of the step performed by gradient descent. Recall from the lectures that if $\\alpha$ is large gradient descent might fail to find the minimum of the error function.   \n",
    "\\begin{align*}\n",
    "w_{i, j} &= w_{i, j} - \\alpha \\frac{\\partial J(w_i)}{\\partial w_{i, j}} \\text{ for all }j \\\\\n",
    "b &= b - \\alpha \\frac{\\partial J(w_i)}{\\partial b}\n",
    "\\end{align*}\n",
    "\n",
    "If implemented correctly, the equations above finds values of $w_i$ and $b$ that correctly classify approximately 85\\% of the test data. The problem with the approach described is that we need to implement 10 different functions, one for each digit. This is not only time consuming but also computationally expensive. Instead, we will discuss a vectorized version of the algorithm described above. \n",
    "\n",
    "<h3> Vectorized Implementation </h3>\n",
    "\n",
    "In our vectorized implementation the model will produce a vector as output, with one entry for each digit. This way we will not need to implement one function $h_{w_i}$ for each digit, we will implement a single function $h_{W}$ for all digits, where $W$ is a matrix containing all vectors $w_i$ as row vectors. In addition to the matrix $W$, we will use a vector $B$ with one entry for each digit. The input images will be provided as a matrix $X$ with one flattened image in each column of $X$. \n",
    "\n",
    "The labels will also be provided as a matrix $Y$ of shape $(10, m)$, where $m$ is the number of instances in the training set. Each column of $Y$ is a one-hot vector (i.e., a vector with one entry equals $1$ and all other entries equal $0$). The entry with value of $1$ in an one-hot vector indicates the digit of the image. \n",
    "\n",
    "Here are all matrices we will use in our vectorized representation.\n",
    "\n",
    "\\begin{align*}\n",
    "X =& \n",
    " \\begin{bmatrix}\n",
    " p_{0,0} &  p_{0, 1} & \\cdots &  p_{0, m-1} \\\\\n",
    " p_{1, 0} &  p_{1,1} & \\cdots &  p_{1, m-1} \\\\\n",
    " \\cdots & \\cdots & \\cdots & \\cdots \\\\\n",
    "  p_{783, 0} &  p_{783, 1} & \\cdots &  p_{783, m-1} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "W =& \n",
    " \\begin{bmatrix}\n",
    "w_{0,0} & w_{0,1} & \\cdots & w_{0,783} \\\\\n",
    "w_{1,0} & w_{1,1} & \\cdots & w_{1,783} \\\\\n",
    "\\cdots & \\cdots & \\cdots & \\cdots \\\\\n",
    "w_{9,0} & w_{9,1} & \\cdots & w_{9,783} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "B =& \n",
    " \\begin{bmatrix}\n",
    "b_0 \\\\\n",
    "b_1 \\\\\n",
    "\\cdots \\\\\n",
    "b_9\n",
    "\\end{bmatrix} \\\\\n",
    "Y =& \n",
    " \\begin{bmatrix}\n",
    " y_{0,0} &  y_{0, 1} & \\cdots &  y_{0, m-1} \\\\\n",
    " y_{1, 0} &  y_{1,1} & \\cdots &  y_{1, m-1} \\\\\n",
    " \\cdots & \\cdots & \\cdots & \\cdots \\\\\n",
    "  y_{9, 0} &  y_{9, 1} & \\cdots & y_{9, m-1} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Using the matrices above we can predict the label of all instances in a data set by performing the following operation: $\\hat{Y} = W.$dot$(X) + B$. The largest number in each column of matrix $\\hat{Y}$ will indicate the predicted label. \n",
    "\n",
    "Complete the code below with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_theta(X, W, B):\n",
    "    \"\"\"\n",
    "    For a given matrix W and vector B, this function predicts the value\n",
    "    of each digit for each image of X. Here we assume that each column of X\n",
    "    is a flattened image. \n",
    "    \"\"\"\n",
    "    return W.dot(X) + B \n",
    "\n",
    "# Learning rate alpha, for controlling the step of gradient descent\n",
    "alpha = 0.01\n",
    "\n",
    "# Number of instances in the training set\n",
    "m = images.shape[1]\n",
    "\n",
    "# Matrix W initialized with zeros\n",
    "W = np.zeros((10, 28*28))\n",
    "\n",
    "# Matrix B also initialized with zeros\n",
    "B = np.zeros((10,1))\n",
    "\n",
    "# Creating Y matrix where each column is an one-hot vector\n",
    "Y = np.zeros((10, m))\n",
    "for index, value in enumerate(labels):\n",
    "    Y[value][index] = 1\n",
    "\n",
    "# Performs 1000 iterations of gradient descent\n",
    "for i in range(1000):\n",
    "    # Write here your implementation of gradient descent\n",
    "    \n",
    "# Once finished performing 1000 gradient descent iterations, then compute the percentage\n",
    "# of images from the test set classified correctly; print the percentage on the screen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the reshaped image of each vector $w_i$ (reshaped from 784 to 28x28) we can see what the model has learned. You should observe that the image of each vector $w_i$ resembles the digit it is trying to recognize. This is because the dot product $x \\cdot w_i$ will be large if $x$ and $w_i$ are similar; the result will be small otherwise. This explains why the image of a vector $w_i$ looks like an average image of many handwritten digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    w = W[i,:].reshape(28, 28)\n",
    "    plot_digit(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
